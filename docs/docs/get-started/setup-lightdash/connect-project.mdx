---
sidebar_position: 3
sidebar_label: Connect a project
---

# Connect a project

Once you've [installed Lightdash](./install-lightdash), you can connect to your existing dbt project:

* [connect to a project on GitHub](#github)
* [connect to a project on Gitlab](#gitlab)
* [connect to a project on your local machine](#local-dbt-project) _(only available for local Lightdash installations)_


Once you've set up the connection to your dbt project, you'll need to continue on to [set up the connection to your warehouse](#warehouse-connection) (it's a short step, we promise 🤞).

We currently support:

1. [Bigquery](#bigquery)
2. [Postgres](#postgres)
3. [Redshift](#redshift)
4. [Snowflake](#snowflake)
5. [Databricks](#databricks)

If we don't support the warehouse you're using, don't be afraid to reach out to us in [GitHub](https://github.com/lightdash/lightdash)! :)

## dbt connection options

----

### GitHub

#### Personal access token
This is used to access your repo.
See the [instructions for creating a personal access token here](https://docs.github.com/en/github/authenticating-to-github/keeping-your-account-and-data-secure/creating-a-personal-access-token).

Select `repo` scope when you're creating the token.

![screenshot](../assets/oauth-scope.png)

#### Repository
This should be in the format `my-org/my-repo`. e.g. `lightdash/lightdash-analytics`

#### Branch
This is the branch in your GitHub repo that Lightdash should sync to. e.g. `main`, `master` or `dev`

By default, we've set this to `main` but you can change it to whatever you'd like.

#### Project directory path
This is the folder where your `dbt_project.yml` file is found in the GitHub repository you entered above.

- Put `/` if your `dbt_project.yml` file is in the main folder of your repo (e.g. lightdash/lightdash-analytics/dbt_project.yml)
- Include the path to the sub-folder where your dbt project is if your dbt project is in a sub-folder in your repo. For example, if my project was in lightdash/lightdash-analytics/dbt/dbt_project.yml, I'd write `/dbt` in this field.

----

### GitLab

#### Personal access token
This is used to access your repo.
See the [instructions for creating a personal access token here](https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html).

Select `read_repository` scope when you're creating the token.

#### Repository
This should be in the format `my-org/my-repo`. e.g. `lightdash/lightdash-analytics`

#### Branch
This is the branch in your GitLab repo that Lightdash should sync to. e.g. `main`, `master` or `dev`

By default, we've set this to `main` but you can change it to whatever you'd like.

#### Project directory path
This is the folder where your `dbt_project.yml` file is found in the GitLab repository you entered above.

If your `dbt_project.yml` file is in the main folder of your repo (e.g. `lightdash/lightdash-analytics/dbt_project.yml`),
then you don't need to change anything in here. You can just leave the default value we've put in.

If your dbt project is in a sub-folder in your repo (e.g. `lightdash/lightdash-analytics/dbt/dbt_project.yml`), then
you'll need to include the path to the sub-folder where your dbt project is (e.g. `/dbt`).

----

### Azure DevOps

#### Personal access token

This is your secret token used to access Azure Devops. See the [instructions to create a personal access token](https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/use-personal-access-tokens-to-authenticate?view=azure-devops&tabs=Windows)
You must specify at least the Repo:Read scope.

#### Organization

This is the name of the organization that owns your repository

#### Project

This is the name of the project that owns your repository

#### Repository

This is the name of the repository. For many projects, this is the same as your project name above.

#### Branch
This is the branch in your repository that Lightdash should sync to. e.g. `main`, `master` or `dev`

By default, we've set this to `main` but you can change it to whatever you'd like.

#### Project directory path
This is the folder where your `dbt_project.yml` file is found in the repository you entered above.

If your `dbt_project.yml` file is in the main folder of your repo (e.g. `lightdash/lightdash-analytics/dbt_project.yml`),
then you don't need to change anything in here. You can just leave the default value we've put in.

If your dbt project is in a sub-folder in your repo (e.g. `lightdash/lightdash-analytics/dbt/dbt_project.yml`), then
you'll need to include the path to the sub-folder where your dbt project is (e.g. `/dbt`).

### Local dbt project

:::caution Prerequisite

Unsuitable for production and only available for Lightdash instances installed on your local machine

:::

To start Lightdash with the option to connect to a local dbt project, you must specify the directory of the dbt project when 
you start docker compose:

```shell
# Specify the absolute path to your dbt project
# e.g. export DBT_PROJECT_DIR=/Users/elonmusk/mydbtproject
export DBT_PROJECT_DIR= # Enter your path here!
docker compose start
```

----

## Warehouse connection

### Bigquery

You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile).

#### Project

This is the GCP project ID.

#### Data set

This is the name of your dbt dataset: the dataset in your warehouse where the output of your dbt models is written to.
If you're not sure what this is, check out the `dataset` value [you've set in your dbt `profiles.yml` file](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile#:~:text=This%20connection%20method%20requires%20local%20OAuth%20via%20gcloud.).

#### Location

The location of BigQuery datasets.
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile#dataset-locations).

#### Key File

This is the JSON key file.
You can see [how to create a key here](https://cloud.google.com/docs/authentication/getting-started#cloud-console)

#### Threads
This is the amount of threads dbt can have with the warehouse.

#### Timeout in seconds
If a dbt model takes longer than this timeout to complete, then BigQuery may cancel the query.
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile#timeouts).

#### Priority
The priority for the BigQuery jobs that dbt executes.
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile#priority).

#### Retries

The number of times dbt should retry queries that result in unhandled server errors
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile#retries).

#### Maximum bytes billed

When a value is configured, queries executed by dbt will fail if they exceed the configured maximum bytes threshold.
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/bigquery-profile#maximum-bytes-billed).

----

### Postgres

You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/postgres-profile).

#### Host

This is the host where the database is running.

#### User

This is the database user name.

#### Password

This is the database user password.

#### DB name

This is the database name.

#### Schema

This is the schema name.

#### Port

This is the port where the database is running.

#### Threads

This is the amount of threads dbt can have with the warehouse.

#### Keep alive idle (seconds)

This specifies the amount of seconds with no network activity after which the operating system should send a TCP keepalive message to the client.
You can see more details in [postgresqlco documentation](https://postgresqlco.nf/doc/en/param/tcp_keepalives_idle/).

#### Search path
This controls the Postgres "search path".
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/postgres-profile#search_path).

#### SSL mode
This controls how dbt connects to Postgres databases using SSL.
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/postgres-profile#sslmode).

----

### Redshift

You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/redshift-profile).

#### Host

This is the host where the database is running.

#### User

This is the database user name.

#### Password

This is the database user password.

#### DB name

This is the database name.

#### Schema

This is the schema name.

#### Port

This is the port where the database is running.

#### Threads

This is the amount of threads dbt can have with the warehouse.
#### Keep alive idle (seconds)

This specifies the amount of seconds with no network activity after which the operating system should send a TCP keepalive message to the client.

#### SSL mode

This controls how dbt connects to Postgres databases using SSL.

#### RA3 Node

Allow dbt to use cross-database-resources

----

### Snowflake

You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/snowflake-profile).

#### Account

This is the account to connect to.

#### User

This is the database user name.

#### Password

This is the database user password.

#### Role

This is the role to assume when running queries as the specified user.

#### Database

This is the database name.

#### Warehouse

This is the warehouse name.

#### Schema

This is the schema name.

#### Threads

This is the amount of threads dbt can have with the warehouse.

#### Keep client session alive

This is intended to keep Snowflake sessions alive beyond the typical 4 hour timeout limit
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/snowflake-profile#client_session_keep_alive).

#### Query tag

This is Snowflake query tags parameter.
You can see more details in [dbt documentation](https://docs.getdbt.com/reference/warehouse-profiles/snowflake-profile#query_tag).

---

### Databricks

The credentials needed to connect to your cluster can be found in the ODBC options in your databricks account:

1. Go to the `Compute` tab in the sidebar.
2. Click the configuration tab for the cluster that you're connecting to Lightdash.
3. Expand the `Advanced options` tab
4. Open the `JDBC/ODBC` tab

![databricks connect screenshot](./assets/databricks-connect-screenshot.png)

#### Server hostname

Follow the instructions above to find your ODBC connection instructions.

#### HTTP Path

Follow the instructions above to find your ODBC connection instructions.

#### Port

Follow the instructions above to find your ODBC connection instructions.

#### Personal Access Token

Your personal access token can be found in your user settings in databricks:

1. Open `Settings` by clicking the cog ⚙️ in the sidebar and select `User settings`
2. Click `Generate token`. You'll be asked to enter a name and expiry.
3. Copy the token

![databricks access screenshot](./assets/databricks-access-screenshot.png)


#### Database

The default database name used by dbt for this connection. In databricks/spark the database is also the schema.


